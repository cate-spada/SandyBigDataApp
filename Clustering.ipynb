{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "501aa39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sarap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sarap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import string \n",
    "import re \n",
    "import pandas as pd\n",
    "import string\n",
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.feature_extraction.text\n",
    "import nltk\n",
    "import geopandas as gpd \n",
    "import geopy \n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from collections import OrderedDict\n",
    "import ssl\n",
    "import certifi\n",
    "from pyspark import F\n",
    "from pyspark.ml.feature import VectorAssembler, StopWordsRemover\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, count, countDistinct, max, when, split, translate\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, NGram\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "sw = nltk.corpus.stopwords;\n",
    "sw._LazyCorpusLoader__load();\n",
    "stopwords = sw.words('english')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e9bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "ss = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = ss.read.options(delimiter=\",\", header=True, inferSchema=True).csv('03_damage-n-casualties_classify-extract/a146281_clean12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b79e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['point']=df2['point'].apply(lambda x: (str(x.latitude)+\",\"+str(x.longitude)) if (x !='Nan' and x!=None) else 'Nan')\n",
    "df3=df2.copy()\n",
    "df3[['latitude', 'longitude']] = df3['point'].str.split(',', 1, expand=True)\n",
    "df3[['address','latitude', 'longitude','point']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3177c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Create dictionary and synonyms\n",
    "dict_w = ['homeless', 'subway flooded', 'without power',\n",
    "              'collapsed crane', 'collapsed boardwalk','research lost',\n",
    "              'nuke plants','trees down','flooding','shelters','deaths','injured','missing']\n",
    "\n",
    "syn_w = {'home': 'homeless','home destroyed':'homeless','subway':'subway flooded',\n",
    "             'subway delayed': 'subway flooded' ,'subway tunnels': 'subway flooded',\n",
    "             'shut down': 'without power' , 'out power': 'without power','the dark': 'without power',\n",
    "             ' power outages': 'without power' ,'wo power': 'without power','in dark': 'without power',\n",
    "             'loses power': 'without power','shutsdown': 'without power','electrical cables': 'without power',\n",
    "             'blackouts': 'without power','lost power': 'without power','crane collapse': 'collapsed crane',\n",
    "             'crane': 'collapsed crane','crane crumples': 'collapsed crane', 'crane collapses': 'collapsed crane',\n",
    "             'crane dangles': 'collapsed crane','documents historical': 'research lost',\n",
    "             'downed tree': 'trees down', 'tree': 'trees down' ,'trees': 'trees down',\n",
    "             'flooded': 'flooding','massive surge': 'flooding','water up': 'flooding',\n",
    "            'flood': 'flooding', 'water debris': 'flooding', 'sewage plant': 'flooding',\n",
    "             'killed people': 'deaths','killed':'deaths','children killed': 'deaths','kills people': 'deaths',\n",
    "             'dead': 'deaths','people died': 'deaths','people killed': 'deaths',\n",
    "             'people dead': 'deaths'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971fd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.where(col(\"longitude\").isNotNull())\n",
    "df2 = df2.where(col(\"latitude\").isNotNull())\n",
    "df2 = df2.filter(~col('longitude').contains('Nan') & ~col('latitude').contains('Nan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1a489",
   "metadata": {},
   "source": [
    "# K-MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b953e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-MEANs\n",
    "K=6\n",
    "df_float = df2.withColumn(\"latitude\", df[\"latitude\"].cast(FloatType()))\\\n",
    "        .withColumn(\"longitude\", df[\"longitude\"].cast(FloatType()))\n",
    "vecAssembler = VectorAssembler(inputCols=[\"latitude\", \"longitude\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df_float)\n",
    "kmeans = KMeans(k=K, seed=1)\n",
    "model = kmeans.fit(new_df.select('features'))\n",
    "transformed = model.transform(new_df)\n",
    "\n",
    "#Inizio for\n",
    "s_found = []\n",
    "color = [\"#FF0000\",\"#00FF00\",\"#0000FF\",\"#FF8000\",\"#7F00FF\",\"#B39500\"]\n",
    "for i in range (K):\n",
    "        cp=transformed.filter(transformed.prediction == i)\n",
    "        #TF-IDF\n",
    "        #Processing data\n",
    "        tokenizer = Tokenizer(inputCol=\"text_no_rt\", outputCol=\"words\")\n",
    "        ngram = NGram(n=2, inputCol=\"words\", outputCol=\"words2\")\n",
    "\n",
    "        pipeline = Pipeline(stages=[tokenizer, ngram])\n",
    "        model = pipeline.fit(cp)\n",
    "        model = model.transform(cp)\n",
    "\n",
    "        df_merged = model.select(f.concat('words', 'words2').alias(\"words3\"))\n",
    "        \n",
    "        \n",
    "        remover = StopWordsRemover(inputCol=\"words3\", outputCol=\"words\")\n",
    "        df_clean = remover.transform(df_merged)\n",
    "       \n",
    "        \n",
    "        vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "        idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "        pipeline = Pipeline(stages=[vectorizer, idf])\n",
    "        model = pipeline.fit(df_clean)\n",
    "\n",
    "        total_counts = model.transform(df_clean).select('rawFeatures').rdd \\\n",
    "        .map(lambda row: row['rawFeatures'].toArray()) \\\n",
    "        .reduce(lambda x, y: [x[i] + y[i] for i in range(len(y))])\n",
    "\n",
    "\n",
    "        vocabList = model.stages[0].vocabulary\n",
    "        d = {'vocabList': vocabList, 'counts': total_counts}\n",
    "\n",
    "        representative = ss.createDataFrame(np.array(list(d.values())).T.tolist(), list(d.keys()))\n",
    "        \n",
    "        \n",
    "        #The most representative ones are compared with a dictionary containing information that allows to identify the subevent\n",
    "    #replace sinonimi con i valori del dizionario\n",
    "        df_replace = representative.rdd.map(lambda x:  (syn_w[x.vocabList],x.counts)\n",
    "                                            if x.vocabList in syn_w else (x.vocabList,x.counts)).toDF([\"value\", \"count\"])\n",
    "     \n",
    "        \n",
    "        #seleziono solo quelle contenute nel vocabolario\n",
    "        df_sel = df_replace.filter(f.col('value').isin(*dict_w))\n",
    "       \n",
    "        \n",
    "        event = df_sel.collect()[0]['value']\n",
    "        points = np.array(cp.select('latitude','longitude').collect())\n",
    "        new_array = [tuple(row) for row in points]\n",
    "        point_uniques = np.unique(new_array,axis=0).tolist()\n",
    "        for point in point_uniques:\n",
    "            s_found.append({\n",
    "                    'event' : event, \n",
    "                    'latitude': point[0],\n",
    "                    'longitude': point[1],\n",
    "                    'color': color[i]\n",
    "            })\n",
    "        return s_found\n",
    "\n",
    "print(s_found)\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8011e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 53461)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python310\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Program Files\\Python310\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Program Files\\Python310\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Program Files\\Python310\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\sarap\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\sarap\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"C:\\Users\\sarap\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"C:\\Users\\sarap\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"C:\\Program Files\\Python310\\lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] Connessione in corso interrotta forzatamente dall'host remoto\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('query16_clustering.txt', 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(s_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1d5cb",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e9519e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words3                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[rt, nowthised, wider, shot, of, scaffolding, toppling, car, on, cpw, and, 92nd, across, from, central, parknyc, nowthisnews, sandy, httptcoivkexinw, rt nowthised, nowthised wider, wider shot, shot of, of scaffolding, scaffolding toppling, toppling car, car on, on cpw, cpw and, and 92nd, 92nd across, across from, from central, central parknyc, parknyc nowthisnews, nowthisnews sandy, sandy httptcoivkexinw]               |\n",
      "|[breaking, new, jersey, sewage, plant, hit, by, sandy, ultimate, shitstorm, imminent, for, the, east, coast, breaking, breaking new, new jersey, jersey sewage, sewage plant, plant hit, hit by, by sandy, sandy ultimate, ultimate shitstorm, shitstorm imminent, imminent for, for the, the east, east coast, coast breaking]                                                                                                        |\n",
      "|[a, boat, lies, across, the, railroad, tracks, near, metronorths, ossining, station, httptco952jfshe, sandy, sandynyc, a boat, boat lies, lies across, across the, the railroad, railroad tracks, tracks near, near metronorths, metronorths ossining, ossining station, station httptco952jfshe, httptco952jfshe sandy, sandy sandynyc]                                                                                               |\n",
      "|[nearly, all, of, weston, without, power, after, hurricane, sandy, tues, am, httptcosfacqmfv, nearly all, all of, of weston, weston without, without power, power after, after hurricane, hurricane sandy, sandy tues, tues am, am httptcosfacqmfv]                                                                                                                                                                                    |\n",
      "|[just, in, nearly, 300000, customers, are, without, power, in, 7, states, where, hurricane, sandy, is, having, an, impact, nj, has, the, most, at, 92000, just in, in nearly, nearly 300000, 300000 customers, customers are, are without, without power, power in, in 7, 7 states, states where, where hurricane, hurricane sandy, sandy is, is having, having an, an impact, impact nj, nj has, has the, the most, most at, at 92000]|\n",
      "|[most, of, manhattan, below, 39st, is, still, without, power, nyc, sandy, most of, of manhattan, manhattan below, below 39st, 39st is, is still, still without, without power, power nyc, nyc sandy]                                                                                                                                                                                                                                   |\n",
      "|[wind, a, big, problem, in, nyc, this, midtown, building, is, losing, glass, police, clearing, streets, now, sandy, httptcoqv6lqizt, wind a, a big, big problem, problem in, in nyc, nyc this, this midtown, midtown building, building is, is losing, losing glass, glass police, police clearing, clearing streets, streets now, now sandy, sandy httptcoqv6lqizt]                                                                   |\n",
      "|[crane, on, 57, street, collapsed, so, dangerous, nyc, hurricane, sandy, httptcos6t3ohtr, crane on, on 57, 57 street, street collapsed, collapsed so, so dangerous, dangerous nyc, nyc hurricane, hurricane sandy, sandy httptcos6t3ohtr]                                                                                                                                                                                              |\n",
      "|[most, of, the, water, in, nyc, had, receded, but, debris, is, everywhere, movement, around, li, is, going, to, be, difficult, at, best, sandy, most of, of the, the water, water in, in nyc, nyc had, had receded, receded but, but debris, debris is, is everywhere, everywhere movement, movement around, around li, li is, is going, going to, to be, be difficult, difficult at, at best, best sandy]                             |\n",
      "|[superstorm, sandy, washes, up, tanker, onto, new, yorks, staten, island, photo, httptcomnxrvjhf, via, gma, superstorm sandy, sandy washes, washes up, up tanker, tanker onto, onto new, new yorks, yorks staten, staten island, island photo, photo httptcomnxrvjhf, httptcomnxrvjhf via, via gma]                                                                                                                                    |\n",
      "|[sandy, storms, across, pennsylvania, leaving, flooded, nyc, in, dark, httptco54sivw57, via, newsmaxmedia, sandy storms, storms across, across pennsylvania, pennsylvania leaving, leaving flooded, flooded nyc, nyc in, in dark, dark httptco54sivw57, httptco54sivw57 via, via newsmaxmedia]                                                                                                                                         |\n",
      "|[sandy, kills, 50, people, nationwide, 18, were, new, york, city, residents, more, than, 8, million, homes, still, without, power, across, 17, states, sandy kills, kills 50, 50 people, people nationwide, nationwide 18, 18 were, were new, new york, york city, city residents, residents more, more than, than 8, 8 million, million homes, homes still, still without, without power, power across, across 17, 17 states]         |\n",
      "|[bloomberg, 34, of, a, million, in, nyc, without, power, sandy, bloomberg 34, 34 of, of a, a million, million in, in nyc, nyc without, without power, power sandy]                                                                                                                                                                                                                                                                     |\n",
      "|[on, infrastructure, aid, , recovery, in, haiti, where, hundreds, of, thousands, go, homeless, after, natural, disasters, httptcocpljlvey, sandy, on infrastructure, infrastructure aid, aid ,  recovery, recovery in, in haiti, haiti where, where hundreds, hundreds of, of thousands, thousands go, go homeless, homeless after, after natural, natural disasters, disasters httptcocpljlvey, httptcocpljlvey sandy]                |\n",
      "|[mt, nygovcuomo, entrance, to, battery, park, flooded, nyc, dot, truck, seen, submerged, httptco90ts0xac, sandy, mt nygovcuomo, nygovcuomo entrance, entrance to, to battery, battery park, park flooded, flooded nyc, nyc dot, dot truck, truck seen, seen submerged, submerged httptco90ts0xac, httptco90ts0xac sandy]                                                                                                               |\n",
      "|[sandy, at, least, 300000, customers, wo, power, in, 7, states, new, jersey, most, outages, w, over, 91500, in, the, dark, cnn, sandy at, at least, least 300000, 300000 customers, customers wo, wo power, power in, in 7, 7 states, states new, new jersey, jersey most, most outages, outages w, w over, over 91500, 91500 in, in the, the dark, dark cnn]                                                                          |\n",
      "|[sandy, left, 250000, people, homeless, in, haiti, the, country, wasnt, prepared, and, now, its, decimated, sandy left, left 250000, 250000 people, people homeless, homeless in, in haiti, haiti the, the country, country wasnt, wasnt prepared, prepared and, and now, now its, its decimated]                                                                                                                                      |\n",
      "|[un, security, council, relocates, due, to, storm, damage, envoys, httptcorqtqpzyv, un security, security council, council relocates, relocates due, due to, to storm, storm damage, damage envoys, envoys httptcorqtqpzyv]                                                                                                                                                                                                            |\n",
      "|[hi, guys, waves, back, rt, time, nyc, firefighters, look, up, at, a, collapsed, crane, dangling, from, a, building, , httptcodcb9fvrf, sandy, hi guys, guys waves, waves back, back rt, rt time, time nyc, nyc firefighters, firefighters look, look up, up at, at a, a collapsed, collapsed crane, crane dangling, dangling from, from a, a building, building ,  httptcodcb9fvrf, httptcodcb9fvrf sandy]                            |\n",
      "|[between, nycwestchesterlong, island, and, new, jersey, there, are, 126, million, customers, without, power, sandy, between nycwestchesterlong, nycwestchesterlong island, island and, and new, new jersey, jersey there, there are, are 126, 126 million, million customers, customers without, without power, power sandy]                                                                                                           |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "#Processing data\n",
    "tokenizer = Tokenizer(inputCol=\"text_no_rt\", outputCol=\"words\")\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"words2\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, ngram])\n",
    "################USARE cp INVECE DI df\n",
    "model = pipeline.fit(df2)\n",
    "model = model.transform(df2)\n",
    "\n",
    "df_merged = model.select(f.concat('words', 'words2').alias(\"words3\"))\n",
    "df_merged.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916ea3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words3\", outputCol=\"words\")\n",
    "df_clean = remover.transform(df_merged)\n",
    "#df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e8a024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|    vocabList|counts|\n",
      "+-------------+------+\n",
      "|        sandy|  76.0|\n",
      "|             |  57.0|\n",
      "|          nyc|  44.0|\n",
      "|       people|  21.0|\n",
      "|        power|  20.0|\n",
      "|             |  18.0|\n",
      "|    people in|  18.0|\n",
      "|        crane|  17.0|\n",
      "|     building|  16.0|\n",
      "|           rt|  16.0|\n",
      "|            1|  15.0|\n",
      "|          new|  14.0|\n",
      "|     power in|  13.0|\n",
      "|       deaths|  13.0|\n",
      "|      without|  12.0|\n",
      "|without power|  12.0|\n",
      "|           ny|  11.0|\n",
      "|        in ny|  11.0|\n",
      "|       in the|  10.0|\n",
      "|    deaths in|  10.0|\n",
      "+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[vectorizer, idf])\n",
    "model = pipeline.fit(df_clean)\n",
    "\n",
    "total_counts = model.transform(df_clean).select('rawFeatures').rdd \\\n",
    "        .map(lambda row: row['rawFeatures'].toArray()) \\\n",
    "        .reduce(lambda x, y: [x[i] + y[i] for i in range(len(y))])\n",
    "\n",
    "\n",
    "vocabList = model.stages[0].vocabulary\n",
    "d = {'vocabList': vocabList, 'counts': total_counts}\n",
    "\n",
    "representative = ss.createDataFrame(np.array(list(d.values())).T.tolist(), list(d.keys()))\n",
    "representative.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae54353",
   "metadata": {},
   "source": [
    "# REPLACE E CONFRONTO CON IL DIZIONARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c45ca0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|          value|count|\n",
      "+---------------+-----+\n",
      "|          sandy| 76.0|\n",
      "|               | 57.0|\n",
      "|            nyc| 44.0|\n",
      "|         people| 21.0|\n",
      "|          power| 20.0|\n",
      "|               | 18.0|\n",
      "|      people in| 18.0|\n",
      "|collapsed crane| 17.0|\n",
      "|       building| 16.0|\n",
      "|             rt| 16.0|\n",
      "|              1| 15.0|\n",
      "|            new| 14.0|\n",
      "|       power in| 13.0|\n",
      "|         deaths| 13.0|\n",
      "|        without| 12.0|\n",
      "|  without power| 12.0|\n",
      "|             ny| 11.0|\n",
      "|          in ny| 11.0|\n",
      "|         in the| 10.0|\n",
      "|      deaths in| 10.0|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The most representative ones are compared with a dictionary containing information that allows to identify the sub-event\n",
    "    #replace sinonimi con i valori del dizionario\n",
    "df_replace = representative.rdd.map(lambda x:  (syn_w[x.vocabList],x.counts) if x.vocabList in syn_w else (x.vocabList,x.counts)).toDF([\"value\", \"count\"])\n",
    "df_replace.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6281f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|          value|count|\n",
      "+---------------+-----+\n",
      "|collapsed crane| 17.0|\n",
      "|         deaths| 13.0|\n",
      "|  without power| 12.0|\n",
      "| subway flooded|  7.0|\n",
      "|       flooding|  5.0|\n",
      "|collapsed crane|  4.0|\n",
      "|       homeless|  4.0|\n",
      "| subway flooded|  3.0|\n",
      "|        injured|  3.0|\n",
      "|        missing|  3.0|\n",
      "|collapsed crane|  2.0|\n",
      "|  research lost|  2.0|\n",
      "|collapsed crane|  2.0|\n",
      "|collapsed crane|  2.0|\n",
      "|  without power|  1.0|\n",
      "|collapsed crane|  1.0|\n",
      "|       flooding|  1.0|\n",
      "|  without power|  1.0|\n",
      "|  without power|  1.0|\n",
      "|         deaths|  1.0|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#seleziono solo quelle contenute nel vocabolario\n",
    "df_sel = df_replace.filter(f.col('value').isin(*dict_w))\n",
    "df_sel.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133d979",
   "metadata": {},
   "source": [
    "# SELEZIONE DELL'EVENTO E DEL CONVEXHULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d865d1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_float' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m event \u001b[38;5;241m=\u001b[39m df_sel\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mdf_float\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(points)\n\u001b[0;32m      4\u001b[0m area \u001b[38;5;241m=\u001b[39m ConvexHull(points)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_float' is not defined"
     ]
    }
   ],
   "source": [
    "event = df_sel.collect()[0]['value']\n",
    "points = np.array(cp.select('latitude','longitude').collect())\n",
    "print(points)\n",
    "area = ConvexHull(points)\n",
    "s_found['event'].append(event)\n",
    "s_found['area'].append(area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eed7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
